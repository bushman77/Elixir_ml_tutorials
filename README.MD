# Elixir ML Tutorials ğŸ§ âœ¨

A practical, script-first course for learning machine learning **in Elixir**, from first principles to real models.

This repo is organized as a growing set of runnable lessons (`.exs` files) plus supporting notes and exercises. The goal is to make ML feel *native* in Elixir: clear data shapes, understandable math, repeatable experiments, and code you can actually read.

---

## What youâ€™ll learn (the roadmap)

This course is designed to build confidence in layers. Every section adds one â€œnew superpowerâ€, while reusing what you already learned.

### Phase 1 â€” Foundations (tensors + gradients)
Youâ€™ll learn:
- What a **tensor** is (shape, dtype, broadcasting)
- How Elixir does numerical computing (Nx)
- What **autodiff** is and how gradients are computed
- How gradient descent actually updates parameters

Planned lessons:
- **01 â€” Linear regression from scratch (Nx + autodiff)**
- 02 â€” Tensors: shapes, types, slicing, broadcasting, reductions
- 03 â€” Loss functions: MSE, cross-entropy (and why they work)
- 04 â€” Optimization: SGD, momentum, Adam (implement + compare)
- 05 â€” Debugging numerics: NaNs, exploding gradients, normalization

### Phase 2 â€” Data and â€œclassic MLâ€
Youâ€™ll learn:
- Data wrangling workflows (dataframes/series)
- Feature engineering and evaluation
- Classic algorithms (regression, clustering, metrics)

Planned lessons:
- 06 â€” Dataframes + dataset hygiene (train/val/test, leakage)
- 07 â€” Logistic regression for classification
- 08 â€” Metrics: accuracy, precision/recall, F1, ROC-AUC
- 09 â€” Clustering: k-means and distance metrics
- 10 â€” Regularization: L1/L2, bias/variance intuition

### Phase 3 â€” Neural nets â€œthe Elixir wayâ€
Youâ€™ll learn:
- How to build networks as composable functions
- Training loops and batching
- Using higher-level APIs when youâ€™re ready

Planned lessons:
- 11 â€” A tiny neural net from scratch (dense layer + backprop)
- 12 â€” Mini-batching + shuffling + dataloaders
- 13 â€” Feedforward classifier on a real dataset
- 14 â€” Convolutions (CNN basics) + image-ish toy data
- 15 â€” Recurrent ideas (sequence batching + masking)

### Phase 4 â€” Performance, serving, and real-world usage
Youâ€™ll learn:
- Backends/JIT concepts and why they matter
- How to run inference cleanly in production-ish setups
- Reproducibility and experiment discipline

Planned lessons:
- 16 â€” Performance basics: vectorization vs loops
- 17 â€” JIT/backends (when/why youâ€™d switch)
- 18 â€” Model saving/loading + versioning
- 19 â€” Inference pipelines & simple â€œservingâ€ patterns
- 20 â€” Capstone: build a complete mini project end-to-end

---

## Current lessons

- `ml_hello_linear_annotated.exs`
  - Linear regression trained with gradient descent
  - Shows `Mix.install`, tensors, `Nx.Defn`, and `value_and_grad`
  - This is **Lesson 01** (foundations)

As the course grows, lessons may be moved into a `lessons/` folder with numbering.
For now, everything is kept simple and runnable from the repo root.

---

## How to run a lesson

Requirements:
- Elixir installed on your system
- Internet access the first time you run a script (to fetch deps via `Mix.install`)

Run:
```bash
elixir ml_hello_linear_annotated.exs
```
